% testsim_cluster_perm_two_conditions_1D_vector_valued_stats
% mainly generated by Claude 4.5 Sonnet

%% Vector-Valued Cluster-Based Permutation Test
% Single subject, between-trials design (following Maris & Oostenveld 2007)

clear; close all; rng(42);

%% ========================================================================
%  PART 1: SIMULATE SINGLE-SUBJECT DATA WITH MULTIPLE TRIALS
%  ========================================================================

fprintf('Generating single-subject trial data...\n');

% Parameters
n_trials_cond1 = 200;      % Number of trials in condition 1
n_trials_cond2 = 150;      % Number of trials in condition 2
n_timepoints = 600;        % Time points per trial (600ms at 1000Hz)
sampling_rate = 1000;      % Hz
time = (0:n_timepoints-1) / sampling_rate;
n_permutations = 1000;


% MINIMUM CLUSTER SIZE (key parameter from the paper)
min_cluster_size = 10;     % Minimum number of time samples to be considered
                           % Clusters smaller than this are excluded as
                           % unlikely to reflect physiological activity

% CLUSTER-FORMING THRESHOLD (p-value for initial thresholding)
cluster_p_thresh = 0.05;   % Two-sided p-value for cluster-forming threshold
                           % Lower = more conservative (smaller clusters)
                           % Higher = more liberal (larger clusters, catches weak effects)

% T-VALUE SMOOTHING (helps merge fragmented clusters)
smooth_t_ms = 20;          % Gaussian smoothing of t-values before thresholding (ms)
                           % Set to 0 to disable; try 5-20 ms to merge nearby effects
smooth_t_samples = round(smooth_t_ms * sampling_rate / 1000);


alpha = 0.05;   % family-wise error rate control

% INFERENCE METHOD
use_vector_valued = true;  % true:  Vector-valued test (Section 4.5 of Maris & Oostenveld 2007)
                           %        Each cluster compared to its dimension-specific null
                           %        (k-th largest observed vs k-th largest under permutation)
                           % false: Standard max-stat test
                           %        All clusters compared to max-cluster null

fprintf('  Minimum cluster size: %d time points (%.1f ms)\n', ...
    min_cluster_size, min_cluster_size/sampling_rate*1000);
fprintf('  Cluster-forming threshold: p < %.3f (two-sided)\n', cluster_p_thresh);
if use_vector_valued
    fprintf('  Inference: Vector-valued (Section 4.5)\n');
else
    fprintf('  Inference: Standard (max-stat only)\n');
end
if smooth_t_ms > 0
    fprintf('  T-value smoothing: %d samples (%.1f ms)\n', smooth_t_samples, smooth_t_ms);
end

% Generate baseline noise for all trials (with temporal smoothing)
noise_std = 1.5;
noise_smooth_ms = 10;  % Gaussian smoothing kernel width (ms) - set to 0 for white noise
noise_smooth_samples = round(noise_smooth_ms * sampling_rate / 1000);

trials_cond1 = noise_std * randn(n_trials_cond1, n_timepoints);
trials_cond2 = noise_std * randn(n_trials_cond2, n_timepoints);

% Apply temporal smoothing to create correlated noise
if noise_smooth_samples > 0
    fprintf('  Applying %.0f ms Gaussian smoothing to noise...\n', noise_smooth_ms);
    for trial = 1:n_trials_cond1
        trials_cond1(trial, :) = smoothdata(trials_cond1(trial, :), 'gaussian', noise_smooth_samples);
    end
    for trial = 1:n_trials_cond2
        trials_cond2(trial, :) = smoothdata(trials_cond2(trial, :), 'gaussian', noise_smooth_samples);
    end
    % Re-scale to maintain original std after smoothing
    trials_cond1 = trials_cond1 * (noise_std / std(trials_cond1(:)));
    trials_cond2 = trials_cond2 * (noise_std / std(trials_cond2(:)));
end

% Add TRUE EFFECTS to condition 2
% The effect is added to the MEAN, so each trial gets a consistent shift

% CLUSTER 1: Early positive effect (100-200ms), strong
cluster1_time = 100:200;
cluster1_amplitude = 0.8;  % Effect on mean
for trial = 1:n_trials_cond2
    trials_cond2(trial, cluster1_time) = trials_cond2(trial, cluster1_time) + cluster1_amplitude;
end

% CLUSTER 2: Mid-latency negative effect (300-450ms), medium
cluster2_time = 300:450;
cluster2_amplitude = -0.6;
for trial = 1:n_trials_cond2
    trials_cond2(trial, cluster2_time) = trials_cond2(trial, cluster2_time) + cluster2_amplitude;
end

% CLUSTER 3: Late positive effect (500-560ms), weaker
cluster3_time = 500:560;
cluster3_amplitude = 0.4;
for trial = 1:n_trials_cond2
    trials_cond2(trial, cluster3_time) = trials_cond2(trial, cluster3_time) + cluster3_amplitude;
end


% CLUSTER 4: Late positive effect (570-590ms), stronger
cluster4_time = 570:590;
cluster4_amplitude = 0.42;
for trial = 1:n_trials_cond2
    trials_cond2(trial, cluster4_time) = trials_cond2(trial, cluster4_time) + cluster4_amplitude;
end

fprintf('  Condition 1: %d trials\n', n_trials_cond1);
fprintf('  Condition 2: %d trials\n', n_trials_cond2);
fprintf('  True effect clusters:\n');
fprintf('    Cluster 1: %d-%d ms (%d samples, amplitude: %.2f)\n', ...
    cluster1_time(1), cluster1_time(end), length(cluster1_time), cluster1_amplitude);
fprintf('    Cluster 2: %d-%d ms (%d samples, amplitude: %.2f)\n', ...
    cluster2_time(1), cluster2_time(end), length(cluster2_time), cluster2_amplitude);
fprintf('    Cluster 3: %d-%d ms (%d samples, amplitude: %.2f)\n', ...
    cluster3_time(1), cluster3_time(end), length(cluster3_time), cluster3_amplitude);
fprintf('    Cluster 4: %d-%d ms (%d samples, amplitude: %.2f)\n', ...
    cluster4_time(1), cluster4_time(end), length(cluster4_time), cluster4_amplitude);

%% ========================================================================
%  PART 2: CALCULATE OBSERVED STATISTICS
%  ========================================================================

fprintf('\nCalculating observed statistics...\n');

% Calculate observed cluster statistics with minimum size filter
[observed_clusters, observed_tvals, n_clusters_before_filter] = ...
    calculate_cluster_stats_between_trials(trials_cond1, trials_cond2, min_cluster_size, smooth_t_samples, cluster_p_thresh);

fprintf('  Detected %d clusters before size filter\n', n_clusters_before_filter);
fprintf('  Retained %d clusters after size filter (>= %d samples)\n', ...
    length(observed_clusters), min_cluster_size);

if isempty(observed_clusters)
    error('No clusters found in observed data! Try lowering min_cluster_size or increasing effect sizes.');
end

% Sort clusters by ABSOLUTE cluster mass (for two-sided test)
[observed_vector, sort_idx] = sort(abs([observed_clusters.mass]), 'descend');
observed_clusters_sorted = observed_clusters(sort_idx);

fprintf('  Observed cluster masses (sorted): ');
fprintf('%.1f ', observed_vector);
fprintf('\n');

%% ========================================================================
%  PART 3: BUILD PERMUTATION DISTRIBUTION
%  ========================================================================

fprintf('\nRunning %d permutations...\n', n_permutations);

% Combine all trials
all_trials = [trials_cond1; trials_cond2];
n_total = size(all_trials, 1);
n_timepoints = size(all_trials, 2);

% PRE-GENERATE ALL RANDOM PARTITION INDICES
fprintf('  Generating %d random partitions...\n', n_permutations);
[~, random_orders] = sort(rand(n_permutations, n_total), 2);
idx_g1 = random_orders(:, 1:n_trials_cond1);
idx_g2 = random_orders(:, n_trials_cond1+1:end);

% Pre-compute constants
df = n_trials_cond1 + n_trials_cond2 - 2;
t_thresh = tinv(1 - cluster_p_thresh/2, df);

% Storage
perm_vectors = cell(n_permutations, 1);
debug_clusters_before = zeros(n_permutations, 1);
debug_clusters_after = zeros(n_permutations, 1);

% SINGLE LOOP: partition → t-test → cluster finding
fprintf('  Computing permutations (t-test + clusters)...\n');
for perm = 1:n_permutations
    % 1. Get trial groups for this partition
    g1 = all_trials(idx_g1(perm,:), :);
    g2 = all_trials(idx_g2(perm,:), :);
    
    % 2. Two-sample t-test (vectorized across timepoints)
    mean1 = mean(g1, 1);
    mean2 = mean(g2, 1);
    var1 = var(g1, 0, 1);
    var2 = var(g2, 0, 1);
    tvals = (mean1 - mean2) ./ sqrt(var1/n_trials_cond1 + var2/n_trials_cond2);
    
    % 3. Smooth if requested
    if smooth_t_samples > 0
        tvals_for_thresh = smoothdata(tvals, 'gaussian', smooth_t_samples);
    else
        tvals_for_thresh = tvals;
    end
    
    % 4. Find clusters (bwconncomp)
    [clusters, n_before] = find_clusters_from_tvals(tvals, tvals_for_thresh, t_thresh, min_cluster_size);
    
    debug_clusters_before(perm) = n_before;
    debug_clusters_after(perm) = length(clusters);
    
    if ~isempty(clusters)
        perm_vectors{perm} = sort(abs([clusters.mass]), 'descend');
    else
        perm_vectors{perm} = [];
    end
end

% DEBUG output
fprintf('\n  DEBUG: Permutation cluster stats:\n');
fprintf('    Clusters before size filter: mean=%.1f, max=%d\n', ...
    mean(debug_clusters_before), max(debug_clusters_before));
fprintf('    Clusters after size filter: mean=%.1f, max=%d\n', ...
    mean(debug_clusters_after), max(debug_clusters_after));
fprintf('    Permutations with >=1 cluster (after filter): %d\n', ...
    sum(debug_clusters_after >= 1));

%% ========================================================================
%  PART 4: BUILD DIMENSION-SPECIFIC DISTRIBUTIONS
%  ========================================================================

fprintf('\nBuilding dimension-specific distributions...\n');

% Determine maximum number of dimensions
max_dimensions = length(observed_vector);
fprintf('  Number of observed clusters (dimensions): %d\n', max_dimensions);

% For each dimension, collect values from permutations
dimension_distributions = cell(max_dimensions, 1);

for dim = 1:max_dimensions
    dim_values = [];
    for perm = 1:n_permutations
        % Only include if this permutation had >= dim clusters
        if length(perm_vectors{perm}) >= dim
            dim_values(end+1) = perm_vectors{perm}(dim);
        end
    end
    dimension_distributions{dim} = dim_values;
    fprintf('  Dimension %d: %d values (%.1f%% of permutations had >= %d clusters)\n', ...
        dim, length(dim_values), 100*length(dim_values)/n_permutations, dim);
end

%% ========================================================================
%  PART 5: CALCULATE CRITICAL VALUES (family-wise error control)
%  ========================================================================

percentile_level = 100 * (1 - alpha);

if use_vector_valued
    %----------------------------------------------------------------------
    % VECTOR-VALUED APPROACH (Section 4.5 of Maris & Oostenveld 2007)
    % k-th largest observed cluster is compared to distribution of 
    % k-th largest clusters under permutation
    %----------------------------------------------------------------------
    fprintf('\nCalculating critical values (VECTOR-VALUED, alpha = %.3f)...\n', alpha);
    
    critical_values = zeros(max_dimensions, 1);
    for dim = 1:max_dimensions
        if ~isempty(dimension_distributions{dim})
            critical_values(dim) = prctile(dimension_distributions{dim}, percentile_level);
        else
            % No permutation ever found this many clusters -> observing them is p≈0
            critical_values(dim) = 0;
        end
    end
    
    % Calculate actual FWER with these critical values
    n_violations = 0;
    for perm = 1:n_permutations
        perm_vec = perm_vectors{perm};
        violation = false;
        for dim = 1:min(length(perm_vec), max_dimensions)
            if perm_vec(dim) > critical_values(dim)
                violation = true;
                break;
            end
        end
        if violation
            n_violations = n_violations + 1;
        end
    end
    actual_fwer = n_violations / n_permutations;
    
    fprintf('  Actual FWER with these critical values: %.4f (target: %.4f)\n', ...
        actual_fwer, alpha);
    fprintf('  Critical values per dimension:\n');
    for dim = 1:max_dimensions
        fprintf('    Dim %d: CV = %.2f (based on %d permutation values)\n', ...
            dim, critical_values(dim), length(dimension_distributions{dim}));
    end
    
else
    %----------------------------------------------------------------------
    % STANDARD APPROACH (Max-stat only)
    % All clusters compared against distribution of MAX cluster under permutation
    %----------------------------------------------------------------------
    fprintf('\nCalculating critical value (STANDARD max-stat, alpha = %.3f)...\n', alpha);
    
    if ~isempty(dimension_distributions{1})
        cv_max = prctile(dimension_distributions{1}, percentile_level);
    else
        cv_max = 0;
    end
    
    % All clusters use the same CV
    critical_values = cv_max * ones(max_dimensions, 1);
    
    % FWER is controlled by construction
    actual_fwer = alpha;
    
    fprintf('  Critical value (from max-cluster null): %.2f\n', cv_max);
    fprintf('  Based on %d permutation values\n', length(dimension_distributions{1}));
end

%% ========================================================================
%  PART 6: TEST EACH OBSERVED CLUSTER
%  ========================================================================

if use_vector_valued
    fprintf('\nTesting observed clusters (vector-valued)...\n');
else
    fprintf('\nTesting observed clusters (standard max-stat)...\n');
end

significant_clusters = false(max_dimensions, 1);
p_values = zeros(max_dimensions, 1);

for dim = 1:max_dimensions
    if use_vector_valued
        % VECTOR-VALUED: p-value from dimension-specific null
        if ~isempty(dimension_distributions{dim})
            n_exceed = sum(dimension_distributions{dim} >= observed_vector(dim));
            p_values(dim) = n_exceed / length(dimension_distributions{dim});
        else
            % No permutation ever found this many clusters -> p = 0
            p_values(dim) = 0;
        end
    else
        % STANDARD: p-value from max-cluster null for all clusters
        if ~isempty(dimension_distributions{1})
            n_exceed = sum(dimension_distributions{1} >= observed_vector(dim));
            p_values(dim) = n_exceed / length(dimension_distributions{1});
        else
            p_values(dim) = 0;
        end
    end
    
    % Test against critical value
    if observed_vector(dim) > critical_values(dim)
        significant_clusters(dim) = true;
        fprintf('  Cluster %d (dim %d): mass=%.2f > CV=%.2f, p=%.4f - SIGNIFICANT\n', ...
            dim, dim, observed_vector(dim), critical_values(dim), p_values(dim));
    else
        fprintf('  Cluster %d (dim %d): mass=%.2f <= CV=%.2f, p=%.4f - Not significant\n', ...
            dim, dim, observed_vector(dim), critical_values(dim), p_values(dim));
    end
end

%% ========================================================================
%  PART 7: VISUALIZATION
%  ========================================================================

fprintf('\nGenerating visualizations...\n');

figure('Position', [100, 50, 1600, 1000]);

% Panel 1: Grand average waveforms with 95% CI
subplot(3, 3, 1);
mean_cond1 = mean(trials_cond1, 1);
mean_cond2 = mean(trials_cond2, 1);
sem_cond1 = std(trials_cond1, 0, 1) / sqrt(n_trials_cond1);
sem_cond2 = std(trials_cond2, 0, 1) / sqrt(n_trials_cond2);
ci95_cond1 = 1.96 * sem_cond1;
ci95_cond2 = 1.96 * sem_cond2;

% Plot CI bands first (so they're behind the lines)
t_ms = time * 1000;
fill([t_ms, fliplr(t_ms)], [mean_cond1 + ci95_cond1, fliplr(mean_cond1 - ci95_cond1)], ...
    'b', 'FaceAlpha', 0.2, 'EdgeColor', 'none'); hold on;
fill([t_ms, fliplr(t_ms)], [mean_cond2 + ci95_cond2, fliplr(mean_cond2 - ci95_cond2)], ...
    'r', 'FaceAlpha', 0.2, 'EdgeColor', 'none');
% Plot mean lines
plot(t_ms, mean_cond1, 'b-', 'LineWidth', 2);
plot(t_ms, mean_cond2, 'r-', 'LineWidth', 2);
xlabel('Time (ms)'); ylabel('Amplitude');
title('Grand Average: Evoked Responses (±95% CI)');
legend('Cond 1 CI', 'Cond 2 CI', 'Condition 1', 'Condition 2', 'Location', 'best');
grid on; box on;

% Panel 2: Difference wave with true cluster locations
subplot(3, 3, 2);
diff_wave = mean_cond2 - mean_cond1;
plot(time*1000, diff_wave, 'k-', 'LineWidth', 2); hold on;
yline(0, 'k--', 'LineWidth', 1);
ylims = ylim;

% Shade TRUE cluster locations
patch([cluster1_time(1), cluster1_time(end), cluster1_time(end), cluster1_time(1)], ...
    [ylims(1), ylims(1), ylims(2), ylims(2)], ...
    'g', 'FaceAlpha', 0.2, 'EdgeColor', 'none');
patch([cluster2_time(1), cluster2_time(end), cluster2_time(end), cluster2_time(1)], ...
    [ylims(1), ylims(1), ylims(2), ylims(2)], ...
    'g', 'FaceAlpha', 0.2, 'EdgeColor', 'none');
patch([cluster3_time(1), cluster3_time(end), cluster3_time(end), cluster3_time(1)], ...
    [ylims(1), ylims(1), ylims(2), ylims(2)], ...
    'g', 'FaceAlpha', 0.2, 'EdgeColor', 'none');
patch([cluster4_time(1), cluster4_time(end), cluster4_time(end), cluster4_time(1)], ...
    [ylims(1), ylims(1), ylims(2), ylims(2)], ...
    'g', 'FaceAlpha', 0.2, 'EdgeColor', 'none');        


xlabel('Time (ms)'); ylabel('Amplitude Difference');
title('Difference Wave (True Clusters = Green)');
grid on; box on;

% Panel 3: T-statistics with threshold
subplot(3, 3, 3);
plot(time*1000, observed_tvals, 'k-', 'LineWidth', 1.5); hold on;

% Calculate degrees of freedom for independent samples t-test
df = n_trials_cond1 + n_trials_cond2 - 2;
t_thresh_plot = tinv(1 - cluster_p_thresh/2, df);  % Two-sided

yline(t_thresh_plot, 'r--', 'LineWidth', 2, 'DisplayName', 'Threshold');
yline(-t_thresh_plot, 'r--', 'LineWidth', 2, 'HandleVisibility', 'off');
yline(0, 'k--', 'LineWidth', 0.5, 'HandleVisibility', 'off');

xlabel('Time (ms)'); ylabel('t-value');
title(sprintf('T-statistics (p<%.2f, t=±%.2f)', cluster_p_thresh, t_thresh_plot));
legend('Location', 'best');
grid on; box on;

% Panel 4: All detected clusters (before significance testing)
subplot(3, 3, 4);
plot(time*1000, diff_wave, 'Color', [0.5 0.5 0.5], 'LineWidth', 1); hold on;
yline(0, 'k--', 'LineWidth', 0.5);
ylims = ylim;

% Show ALL clusters (color-coded by rank)
colors = jet(length(observed_clusters_sorted));
for i = 1:length(observed_clusters_sorted)
    cluster = observed_clusters_sorted(i);
    t_start = cluster.timepoints(1);
    t_end = cluster.timepoints(end);
    
    patch([t_start, t_end, t_end, t_start], ...
        [ylims(1), ylims(1), ylims(2), ylims(2)], ...
        colors(i,:), 'FaceAlpha', 0.4, 'EdgeColor', 'k', 'LineWidth', 1);
    
    % Label with cluster number and size
    text(mean([t_start, t_end]), ylims(2)*0.9, sprintf('%d\n(%d)', i, cluster.size), ...
        'HorizontalAlignment', 'center', 'FontWeight', 'bold', 'FontSize', 9);
end

xlabel('Time (ms)'); ylabel('Amplitude Difference');
title(sprintf('All %d Detected Clusters (>=%d samples)', ...
    length(observed_clusters_sorted), min_cluster_size));
grid on; box on;

% Panel 5: SIGNIFICANT clusters only
subplot(3, 3, 5);
plot(time*1000, diff_wave, 'k-', 'LineWidth', 2); hold on;
yline(0, 'k--', 'LineWidth', 0.5);
ylims = ylim;

% Shade only SIGNIFICANT clusters
n_sig = sum(significant_clusters);
for i = 1:length(observed_clusters_sorted)
    if significant_clusters(i)
        cluster = observed_clusters_sorted(i);
        t_start = cluster.timepoints(1);
        t_end = cluster.timepoints(end);
        
        patch([t_start, t_end, t_end, t_start], ...
            [ylims(1), ylims(1), ylims(2), ylims(2)], ...
            'r', 'FaceAlpha', 0.4, 'EdgeColor', 'none');
        
        text(mean([t_start, t_end]), ylims(2)*0.9, ...
            sprintf('C%d\np=%.4f', i, p_values(i)), ...
            'HorizontalAlignment', 'center', 'FontWeight', 'bold', ...
            'FontSize', 10, 'Color', 'r');
    end
end

xlabel('Time (ms)'); ylabel('Amplitude Difference');
title(sprintf('SIGNIFICANT Clusters (n=%d, alpha=%.3f, min_size=%d)', ...
    n_sig, alpha, min_cluster_size));
grid on; box on;

% Panels 6-9: Dimension-specific distributions (up to 4 dimensions)
for dim = 1:min(4, max_dimensions)
    subplot(3, 3, 5 + dim);
    
    if ~isempty(dimension_distributions{dim})
        % Plot histogram
        histogram(dimension_distributions{dim}, 50, ...
            'FaceColor', [0.7, 0.7, 0.7], 'EdgeColor', 'k', ...
            'Normalization', 'probability');
        hold on;
        
        % Mark critical value
        yl = ylim;
        plot([critical_values(dim), critical_values(dim)], yl, ...
            'r--', 'LineWidth', 2.5, 'DisplayName', sprintf('CV=%.1f', critical_values(dim)));
        
        % Mark observed value
        plot([observed_vector(dim), observed_vector(dim)], yl, ...
            'b-', 'LineWidth', 2.5, 'DisplayName', sprintf('Obs=%.1f', observed_vector(dim)));
        
        xlabel('Absolute Cluster Mass');
        ylabel('Probability');
        
        sig_str = '';
        if dim <= length(significant_clusters) && significant_clusters(dim)
            sig_str = ' - SIGNIFICANT';
        end
        
        title(sprintf('Dimension %d: p=%.4f%s', dim, p_values(dim), sig_str));
        legend('Location', 'best');
        grid on; box on;
    end
end

sgtitle(sprintf('Vector-Valued Cluster-Based Permutation Test (min_cluster_size = %d samples)', ...
    min_cluster_size), 'FontSize', 16, 'FontWeight', 'bold');

%% ========================================================================
%  PART 8: SUMMARY TABLE
%  ========================================================================

fprintf('\n================================================================================\n');
fprintf('SUMMARY OF RESULTS\n');
fprintf('================================================================================\n');
fprintf('Parameters:\n');
if use_vector_valued
    fprintf('  Inference method: VECTOR-VALUED (Section 4.5, Maris & Oostenveld 2007)\n');
else
    fprintf('  Inference method: STANDARD (max-stat only)\n');
end
fprintf('  Minimum cluster size: %d time points (%.1f ms)\n', min_cluster_size, min_cluster_size/sampling_rate*1000);
fprintf('  Alpha level: %.3f\n', alpha);
fprintf('  Number of permutations: %d\n', n_permutations);
fprintf('  Actual FWER: %.4f\n', actual_fwer);
fprintf('--------------------------------------------------------------------------------\n');
fprintf('Cluster | Dim | Time Range (ms) | Size | Mass    | CV      | p-value | Significant\n');
fprintf('--------|-----|-----------------|------|---------|---------|---------|------------\n');

for i = 1:length(observed_clusters_sorted)
    cluster = observed_clusters_sorted(i);
    time_start = cluster.timepoints(1);
    time_end = cluster.timepoints(end);
    cluster_size = cluster.size;
    
    if significant_clusters(i)
        sig_str = 'YES';
    else
        sig_str = 'NO';
    end
    
    fprintf('   %2d   | %2d  |   %3d - %3d     | %4d | %7.1f | %7.1f | %.5f |     %s\n', ...
        i, i, time_start, time_end, cluster_size, ...
        observed_vector(i), critical_values(i), p_values(i), sig_str);
end

fprintf('================================================================================\n');
fprintf('Total clusters found: %d\n', length(observed_clusters_sorted));
fprintf('Significant clusters: %d (alpha = %.3f, FWER controlled)\n', sum(significant_clusters), alpha);
fprintf('Clusters filtered out (< %d samples): %d\n', min_cluster_size, n_clusters_before_filter - length(observed_clusters_sorted));
fprintf('================================================================================\n\n');

%% ========================================================================
%  HELPER FUNCTION: Calculate cluster statistics for between-trials design
%  ========================================================================

function [clusters, tvals, n_before_filter] = calculate_cluster_stats_between_trials(...
    trials_cond1, trials_cond2, min_cluster_size, smooth_t_samples, cluster_p_thresh)
    % Independent samples t-test at each time point (vectorized)
    % Uses bwconncomp for efficient cluster detection
    
    n1 = size(trials_cond1, 1);
    n2 = size(trials_cond2, 1);
    
    % Vectorized two-sample t-test
    mean1 = mean(trials_cond1, 1);
    mean2 = mean(trials_cond2, 1);
    var1 = var(trials_cond1, 0, 1);
    var2 = var(trials_cond2, 0, 1);
    tvals = (mean1 - mean2) ./ sqrt(var1/n1 + var2/n2);
    
    % Optional smoothing
    if smooth_t_samples > 0
        tvals_for_thresh = smoothdata(tvals, 'gaussian', smooth_t_samples);
    else
        tvals_for_thresh = tvals;
    end
    
    % Threshold
    df = n1 + n2 - 2;
    t_thresh = tinv(1 - cluster_p_thresh/2, df);
    
    % Find clusters using bwconncomp
    [clusters, n_before_filter] = find_clusters_from_tvals(tvals, tvals_for_thresh, t_thresh, min_cluster_size);
end

function [clusters, n_before_filter] = find_clusters_from_tvals(tvals, tvals_for_thresh, t_thresh, min_cluster_size)
    % Find clusters from pre-computed t-values using bwconncomp (vectorized)
    
    % Get positive and negative clusters
    pos_mask = tvals_for_thresh > t_thresh;
    neg_mask = tvals_for_thresh < -t_thresh;
    
    % Use bwconncomp for efficient connected component labeling
    CC_pos = bwconncomp(pos_mask);
    CC_neg = bwconncomp(neg_mask);
    
    n_before_filter = CC_pos.NumObjects + CC_neg.NumObjects;
    
    % Combine all pixel lists
    all_pix = [CC_pos.PixelIdxList, CC_neg.PixelIdxList];
    
    if isempty(all_pix)
        clusters = struct('timepoints', {}, 'mass', {}, 'size', {});
        return;
    end
    
    % Vectorized: compute sizes and masses for all clusters at once
    sizes = cellfun(@length, all_pix);
    masses = cellfun(@(idx) sum(tvals(idx)), all_pix);
    
    % Filter by minimum size (vectorized)
    keep = sizes >= min_cluster_size;
    
    % Build output struct array
    n_keep = sum(keep);
    if n_keep == 0
        clusters = struct('timepoints', {}, 'mass', {}, 'size', {});
        return;
    end
    
    kept_pix = all_pix(keep);
    kept_sizes = sizes(keep);
    kept_masses = masses(keep);
    
    % Pre-allocate struct array
    clusters(n_keep) = struct('timepoints', [], 'mass', [], 'size', []);
    for i = 1:n_keep
        clusters(i).timepoints = kept_pix{i}(:)';  % Row vector of indices
        clusters(i).mass = kept_masses(i);
        clusters(i).size = kept_sizes(i);
    end
end
